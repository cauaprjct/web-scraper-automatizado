# ğŸ•·ï¸ Web Scraper Automatizado com E-mail

Um sistema completo de web scraping automatizado que coleta dados de produtos, monitora preÃ§os e envia relatÃ³rios por e-mail. Ideal para monitoramento de ofertas, anÃ¡lise de mercado e automaÃ§Ã£o de tarefas de coleta de dados.

## ğŸš€ Funcionalidades

- **Web Scraping Inteligente**: Coleta dados de mÃºltiplos sites (Amazon, eBay, etc.)
- **Monitoramento de PreÃ§os**: Detecta mudanÃ§as de preÃ§os e ofertas especiais
- **Envio AutomÃ¡tico de E-mails**: RelatÃ³rios HTML personalizados
- **Dashboard Web**: Interface Streamlit para visualizaÃ§Ã£o e configuraÃ§Ã£o
- **Banco de Dados**: Armazenamento persistente com SQLAlchemy
- **Agendamento**: ExecuÃ§Ã£o automÃ¡tica com APScheduler
- **Logging AvanÃ§ado**: Monitoramento completo das operaÃ§Ãµes
- **PrÃ¡ticas Ã‰ticas**: Respeita robots.txt e implementa rate limiting

## ğŸ“‹ PrÃ©-requisitos

- Python 3.8+
- Conta de e-mail para envio de relatÃ³rios
- ConexÃ£o com internet

## ğŸ› ï¸ InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/cauaprjct/web-scraper-automatizado.git
cd web-scraper-automatizado
```

2. Crie um ambiente virtual:
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

3. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

4. Configure as variÃ¡veis de ambiente:
```bash
cp .env.example .env
# Edite o arquivo .env com suas configuraÃ§Ãµes
```

## âš™ï¸ ConfiguraÃ§Ã£o

Edite o arquivo `.env` com suas configuraÃ§Ãµes:

```env
# ConfiguraÃ§Ãµes de E-mail
EMAIL_HOST=smtp.gmail.com
EMAIL_PORT=587
EMAIL_USER=seu-email@gmail.com
EMAIL_PASSWORD=sua-senha-de-app
EMAIL_RECIPIENTS=destinatario@email.com

# ConfiguraÃ§Ãµes de Scraping
SCRAPING_DELAY=2.0
MAX_RETRIES=3
REQUEST_TIMEOUT=30

# Banco de Dados
DATABASE_URL=sqlite:///data/scraper_data.db
```

## ğŸš€ Uso

### Linha de Comando

```bash
# Scraping bÃ¡sico
python main.py --site amazon --search "notebook gamer" --max-price 3000

# Com envio de e-mail
python main.py --site amazon --search "smartphone" --send-email

# MÃºltiplos filtros
python main.py --site ebay --search "tablet" --min-price 500 --max-price 1500 --max-results 20
```

### Dashboard Web

```bash
# Iniciar dashboard
python run_dashboard.py

# Acessar: http://localhost:8501
```

### Agendamento AutomÃ¡tico

```bash
# Executar agendador
python scheduler.py
```

## ğŸ“Š Sites Suportados

- **Amazon** (amazon.com.br)
- **eBay** (ebay.com)
- **Mercado Livre** (mercadolivre.com.br)

## ğŸ¯ Exemplos de Uso

### Monitoramento de PreÃ§os
```bash
python main.py --site amazon --search "iphone 13" --max-price 4000 --send-email
```

### AnÃ¡lise de Mercado
```bash
python main.py --site mercadolivre --search "notebook" --min-price 2000 --max-price 5000 --max-results 50
```

### RelatÃ³rio DiÃ¡rio
```bash
python main.py --site ebay --search "smartwatch" --send-email --save-to-db
```

## ğŸ“ Estrutura do Projeto

```
web-scraper-automatizado/
â”œâ”€â”€ config/              # ConfiguraÃ§Ãµes
â”œâ”€â”€ scrapers/            # MÃ³dulos de scraping
â”œâ”€â”€ database/            # Modelos e gerenciamento de BD
â”œâ”€â”€ email_service/       # Sistema de e-mail
â”œâ”€â”€ dashboard/           # Interface Streamlit
â”œâ”€â”€ utils/               # UtilitÃ¡rios
â”œâ”€â”€ tests/               # Testes unitÃ¡rios
â”œâ”€â”€ main.py              # Script principal
â”œâ”€â”€ scheduler.py         # Agendador
â””â”€â”€ requirements.txt     # DependÃªncias
```

## ğŸ”§ Desenvolvimento

### Executar Testes
```bash
python -m pytest tests/
```

### Adicionar Novo Site
1. Crie um novo scraper em `scrapers/`
2. Herde de `BaseScraper`
3. Implemente `search_products()` e `_parse_product_element()`
4. Registre no `ScraperFactory`

### Exemplo de Novo Scraper
```python
from .base_scraper import BaseScraper

class NovoSiteScraper(BaseScraper):
    def __init__(self, settings):
        super().__init__(settings)
        self.base_url = "https://novosite.com"
    
    async def search_products(self, search_term, **filters):
        # Implementar lÃ³gica de scraping
        pass
```

## ğŸ“§ Sistema de E-mail

O sistema envia relatÃ³rios HTML com:
- Resumo de produtos encontrados
- EstatÃ­sticas de preÃ§os
- GrÃ¡ficos e visualizaÃ§Ãµes
- Anexos CSV/JSON com dados completos

## ğŸ—„ï¸ Banco de Dados

Armazena:
- HistÃ³rico de produtos
- MudanÃ§as de preÃ§os
- SessÃµes de scraping
- Logs de e-mail

## âš¡ Performance

- Rate limiting configurÃ¡vel
- Retry automÃ¡tico em falhas
- Cache de requisiÃ§Ãµes
- Processamento assÃ­ncrono

## ğŸ›¡ï¸ PrÃ¡ticas Ã‰ticas

- Respeita robots.txt
- Implementa delays entre requisiÃ§Ãµes
- User-Agent apropriado
- NÃ£o sobrecarrega servidores

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature
3. Commit suas mudanÃ§as
4. Push para a branch
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ†˜ Suporte

Se encontrar problemas:
1. Verifique a documentaÃ§Ã£o
2. Consulte as [Issues](https://github.com/cauaprjct/web-scraper-automatizado/issues)
3. Abra uma nova issue se necessÃ¡rio

## ğŸ¯ Roadmap

- [ ] Suporte a mais sites de e-commerce
- [ ] IntegraÃ§Ã£o com APIs de notificaÃ§Ã£o (Telegram, Slack)
- [ ] Machine Learning para anÃ¡lise de tendÃªncias
- [ ] Interface web mais avanÃ§ada
- [ ] Suporte a proxies rotativos
- [ ] AnÃ¡lise de sentimentos em reviews

---

**Desenvolvido com â¤ï¸ para automaÃ§Ã£o de coleta de dados**